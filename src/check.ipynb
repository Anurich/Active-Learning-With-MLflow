{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import CONFIG\n",
    "import pandas as pd\n",
    "import os \n",
    "import string \n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "import re \n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# path = \"../datasets/IMDB Dataset.csv\"\n",
    "# df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {idx: word for idx, word in enumerate(set(df[\"sentiment\"].values))}\n",
    "word2index = {word: idx for idx, word in index2word.items()}\n",
    "\n",
    "df[\"label\"] = df[\"sentiment\"].apply(lambda x: word2index[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    sentence = sentence.translate(str.maketrans(\"\",\"\", string.punctuation)).lower()\n",
    "    #let's remove if their any links \n",
    "    sentence = re.sub(r\"https?://\\s+\", \"\", sentence)\n",
    "    sentence = re.sub(r\"\\b\\d+\\b\",  \"\", sentence)\n",
    "    sentence = re.sub(r\" +\",\" \",sentence)\n",
    "    return sentence\n",
    "df[\"preprocessedREVIEW\"] = df[\"review\"].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, sentence, label) -> None:\n",
    "        self.sentences = sentence\n",
    "        self.labels  = label\n",
    "        self.window_size = 510\n",
    "    def _getpadding(self, input_ids, attention_mask):\n",
    "        # let's add the special character \n",
    "        input_ids = [0] + input_ids.cpu().tolist() if isinstance(input_ids, torch.Tensor) else input_ids +[2]\n",
    "        attention_mask = [1] + attention_mask.cpu().tolist() if isinstance(attention_mask, torch.Tensor) else attention_mask +[1]\n",
    "\n",
    "        # let's check if the length is less than the disered length in that case we change the length \n",
    "        pad_length = self.window_size - len(input_ids) +2\n",
    "\n",
    "        if pad_length > 0:\n",
    "            # we need to pad \n",
    "            input_ids += [0]*pad_length\n",
    "            attention_mask += [0]*pad_length\n",
    "        assert len(input_ids) == len(attention_mask)\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        # let's tokenize\n",
    "        tokenizedText = tokenizer.encode_plus(text, add_special_tokens=False)\n",
    "        # let's padd the tokens\n",
    "        input_ids = tokenizedText[\"input_ids\"]\n",
    "        attention_mask = tokenizedText[\"attention_mask\"]\n",
    "        if len(input_ids) > 510:\n",
    "            splittedInput_ids = torch.tensor(input_ids).split(510)\n",
    "            splittedInput_mask = torch.tensor(attention_mask).split(510)\n",
    "            for inpID, inpMask in zip(splittedInput_ids, splittedInput_mask):\n",
    "                assert len(inpID) == len(inpMask)\n",
    "                input_ids, attention_mask = self._getpadding(inpID, inpMask)\n",
    "                return idx, {\n",
    "                    \"input_ids\": torch.tensor(input_ids),\n",
    "                    \"attention_mask\": torch.tensor(attention_mask),\n",
    "                    \"labels\": label\n",
    "                } \n",
    "        else:    \n",
    "            input_ids, attention_mask = self._getpadding(input_ids, attention_mask)\n",
    "            return idx, {\n",
    "                \"input_ids\": torch.tensor(input_ids),\n",
    "                \"attention_mask\": torch.tensor(attention_mask),\n",
    "                \"labels\": label\n",
    "            }\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class ActiveLearning:\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.unlabelledDataset = dataset\n",
    "        self.labelledDataset = None\n",
    "\n",
    "    def randomSample(self, k = 200):\n",
    "        labelledIndexes= random.sample(range(len(self.unlabelledDataset)), k=200)\n",
    "        unlabelledIndexes = range(len(self.unlabelledDataset))\n",
    "        # let's remove the labelled Indexes from unlabelled Indexes\n",
    "        unlabelledIndexes = list(filter(lambda x: x not in labelledIndexes, unlabelledIndexes))\n",
    "        \n",
    "        self.labelledDataset = torch.utils.data.Subset(self.unlabelledDataset, labelledIndexes)\n",
    "        self.unlabelledDataset= torch.utils.data.Subset(self.unlabelledDataset, unlabelledIndexes)\n",
    "        # now we have the labelled dataset we can substract the labelled datset from unlabelled one\n",
    "    \n",
    "    def getlabelledDataset(self, labelledIndexes):\n",
    "        unlabelledIndexes = range(len(self.unlabelledDataset))\n",
    "        unlabelledIndexes = list(filter(lambda x: x not in labelledIndexes, unlabelledIndexes))\n",
    "        self.labelledDataset = torch.utils.data.ConcatDataset([self.labelledDataset, torch.utils.data.Subset(self.unlabelledDataset, labelledIndexes)])\n",
    "        self.unlabelledDataset= torch.utils.data.Subset(self.unlabelledDataset, unlabelledIndexes)\n",
    "\n",
    "    @property\n",
    "    def get_unlabelled_dataset(self):\n",
    "        return len(self.unlabelledDataset)\n",
    "    \n",
    "    @property\n",
    "    def get_labelled_dataset(self):\n",
    "        return len(self.labelledDataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k fold cross validation\n",
    "class dataSplitting:\n",
    "    def __init__(self, dataset, k=4) -> None:\n",
    "        self.stratkFold = StratifiedKFold(n_splits=k)\n",
    "        self.dataset = dataset\n",
    "        self.multifold = \"../datasets/\"\n",
    "    def splitData(self):\n",
    "        X = self.dataset[\"preprocessedREVIEW\"]\n",
    "        Y = self.dataset[\"label\"]\n",
    "        for idx, (train_idx, test_idx) in enumerate(self.stratkFold.split(X,Y)):\n",
    "            # let's create folder\n",
    "            path = os.path.join(self.multifold, f\"fold_{idx}\")\n",
    "            if not os.path.isdir(path):\n",
    "                os.mkdir(path)\n",
    "            # let's save the train and test data inside the corresponding fold \n",
    "            train = self.dataset.iloc[train_idx]\n",
    "            test  = self.dataset.iloc[test_idx]\n",
    "            train.to_csv(os.path.join(path, \"train.csv\"), index=False)\n",
    "            test.to_csv(os.path.join(path, \"test.csv\"), index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mfolds  = dataSplitting(df)\n",
    "\n",
    "mfolds.splitData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = dataset(df[\"preprocessedREVIEW\"], df[\"label\"])\n",
    "acv = ActiveLearning(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "acv.randomSample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 49800)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acv.get_labelled_dataset, acv.get_unlabelled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = list(range(20))\n",
    "acv.getlabelledDataset(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[24483,  4198,    16,  ...,     0,     0,     0],\n",
       "         [    0,  1264,    34,  ..., 41307,   784,     0],\n",
       "         [  118,   303,     5,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 9226,    16,    10,  ...,     0,     0,     0],\n",
       "         [  118,    33,    45,  ...,     0,     0,     0],\n",
       "         [ 9226,    16,    30,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 1])}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_ = torch.utils.data.DataLoader(acv.labelledDataset, batch_size=32)\n",
    "next(iter(add_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class strategies:\n",
    "    def __init__(self, model) -> None:\n",
    "        self.model = model\n",
    "\n",
    "    def predict_proba(self, dataloader, rows):\n",
    "        self.model.eval()\n",
    "        probs = []\n",
    "        data = torch.ones([rows, 2])\n",
    "        start = 0\n",
    "        with torch.no_grad():\n",
    "            for  element in tqdm(dataloader):\n",
    "                out = self.model(**element)\n",
    "                pred = torch.softmax(out.logits, dim=-1)\n",
    "                end = start + element[\"input_ids\"].shape[0]\n",
    "                data[start:end] = pred\n",
    "                start = end\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "    def entropySampling(self, unlabelledDataset):\n",
    "        dataloader = torch.utils.data.DataLoader(unlabelledDataset, batch_size=32,shuffle=False)\n",
    "        probs =  self.predict_proba(dataloader, len(unlabelledDataset))\n",
    "        log_prob = torch.log(probs)\n",
    "        return (-probs * log_prob).sum(1)\n",
    "\n",
    "\n",
    "    def entropySamplignDropout(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot = next(iter(add_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x1646db610>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:52<00:00,  7.51s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0.6888),\n",
       " tensor(0.6887),\n",
       " tensor(0.6887),\n",
       " tensor(0.6887),\n",
       " tensor(0.6887),\n",
       " tensor(0.6887),\n",
       " tensor(0.6886),\n",
       " tensor(0.6886),\n",
       " tensor(0.6886),\n",
       " tensor(0.6885),\n",
       " tensor(0.6885),\n",
       " tensor(0.6885),\n",
       " tensor(0.6885),\n",
       " tensor(0.6884),\n",
       " tensor(0.6884),\n",
       " tensor(0.6884),\n",
       " tensor(0.6884),\n",
       " tensor(0.6884),\n",
       " tensor(0.6884),\n",
       " tensor(0.6884),\n",
       " tensor(0.6884),\n",
       " tensor(0.6884),\n",
       " tensor(0.6883),\n",
       " tensor(0.6883),\n",
       " tensor(0.6883),\n",
       " tensor(0.6883),\n",
       " tensor(0.6883),\n",
       " tensor(0.6882),\n",
       " tensor(0.6882),\n",
       " tensor(0.6882),\n",
       " tensor(0.6882),\n",
       " tensor(0.6882),\n",
       " tensor(0.6882),\n",
       " tensor(0.6882),\n",
       " tensor(0.6882),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6881),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6880),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6879),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6878),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6877),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6876),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6875),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6874),\n",
       " tensor(0.6873),\n",
       " tensor(0.6873),\n",
       " tensor(0.6873),\n",
       " tensor(0.6873),\n",
       " tensor(0.6873),\n",
       " tensor(0.6873),\n",
       " tensor(0.6873),\n",
       " tensor(0.6873),\n",
       " tensor(0.6873),\n",
       " tensor(0.6873),\n",
       " tensor(0.6873),\n",
       " tensor(0.6872),\n",
       " tensor(0.6872),\n",
       " tensor(0.6872),\n",
       " tensor(0.6872),\n",
       " tensor(0.6872),\n",
       " tensor(0.6872),\n",
       " tensor(0.6871),\n",
       " tensor(0.6871),\n",
       " tensor(0.6871),\n",
       " tensor(0.6871),\n",
       " tensor(0.6871),\n",
       " tensor(0.6871),\n",
       " tensor(0.6870),\n",
       " tensor(0.6870),\n",
       " tensor(0.6870),\n",
       " tensor(0.6870),\n",
       " tensor(0.6869),\n",
       " tensor(0.6869),\n",
       " tensor(0.6869),\n",
       " tensor(0.6869),\n",
       " tensor(0.6868),\n",
       " tensor(0.6868),\n",
       " tensor(0.6868),\n",
       " tensor(0.6868),\n",
       " tensor(0.6867),\n",
       " tensor(0.6867),\n",
       " tensor(0.6865),\n",
       " tensor(0.6862)]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = strategies(model)\n",
    "ot = st.entropySampling(acv.labelledDataset)\n",
    "sorted(ot, reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acv.get_labelled_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
